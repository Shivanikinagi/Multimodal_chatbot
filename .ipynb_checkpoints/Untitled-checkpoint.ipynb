{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d92025d-f3ee-4cc3-b952-dfa6ad83aac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shivani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shivani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shivani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "import numpy as np \n",
    "from langchain.embeddings import HuggingFaceEmbeddings \n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "import re\n",
    "\n",
    "nltk.download('punkt')  \n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74c1ead8-1ea7-40b4-b121-cfd19e6d266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    important_words = {\"is\", \"as\", \"it\", \"are\", \"has\", \"was\", \"were\", \"be\", \"in\", \"to\", \"the\", \"a\", \"an\", \"that\", \"which\", \"by\", \"from\", \"for\"}\n",
    "    filtered_stop_words = stop_words - important_words \n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(clean_sentence)\n",
    "        words = [lemmatizer.lemmatize(word) if word.lower() not in important_words else word for word in words]\n",
    "        filtered_words = [word for word in words if word.lower() not in filtered_stop_words]\n",
    "        cleaned_sentences.append(' '.join(filtered_words))\n",
    "        \n",
    "    return cleaned_sentences, sentences  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e14cf8b3-a42d-4831-b3e7-6317620f7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences(text, processed_sentences, original_sentences, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    text_embedding = embeddings.embed_query(text)  # Embed the entire text\n",
    "    sentence_embeddings = embeddings.embed_documents(processed_sentences)  # Embed each processed sentence\n",
    "    \n",
    "    position_weights = [\n",
    "        1.2 if i == 0 or i == len(processed_sentences) - 1 else  \n",
    "        1.1 if i == 1 or i == len(processed_sentences) - 2 else  \n",
    "        1.0 for i in range(len(processed_sentences))  \n",
    "    ]\n",
    "    \n",
    "    sentence_scores = []  \n",
    "    for i, sentence_embedding in enumerate(sentence_embeddings):\n",
    "        similarity = np.dot(text_embedding, sentence_embedding) / (np.linalg.norm(text_embedding) * np.linalg.norm(sentence_embedding))\n",
    "        weighted_score = similarity * position_weights[i]\n",
    "        sentence_scores.append(weighted_score) \n",
    "    \n",
    "    ranked_indices = sorted(range(len(sentence_scores)), key=lambda i: sentence_scores[i], reverse=True)\n",
    "    ranked_sentences = [original_sentences[i] for i in ranked_indices]  \n",
    "    \n",
    "    return ranked_sentences  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3b25d4e-0dd6-4857-88cf-8e3de6eac2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_sentences(text, top_k=5, ensure_diversity=True, diversity_threshold=0.7):\n",
    "    processed_sentences, original_sentences = preprocess_text(text) \n",
    "    \n",
    "    if top_k > len(processed_sentences):\n",
    "        top_k = len(processed_sentences)  \n",
    "    \n",
    "    ranked_sentences = rank_sentences(text, processed_sentences, original_sentences)  \n",
    "    \n",
    "    if ensure_diversity:\n",
    "        summary_sentences = []\n",
    "        used_embeddings = []\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        \n",
    "        for sentence in ranked_sentences:\n",
    "            sentence_embedding = embeddings.embed_query(sentence)  \n",
    "    \n",
    "            is_redundant = any(np.dot(sentence_embedding, used_embedding) / (np.linalg.norm(sentence_embedding) * np.linalg.norm(used_embedding)) > diversity_threshold for used_embedding in used_embeddings)\n",
    "            \n",
    "            if not is_redundant:\n",
    "                summary_sentences.append(sentence)  \n",
    "                used_embeddings.append(sentence_embedding) \n",
    "                if len(summary_sentences) >= top_k:\n",
    "                    break  \n",
    "    else:\n",
    "        summary_sentences = ranked_sentences[:top_k]  \n",
    "    \n",
    "    return summary_sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e97fa5db-ce51-49c6-b723-034bfb497055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text, summary_length=5, ensure_diversity=True, diversity_threshold=0.7):\n",
    "    if summary_length <= 0:\n",
    "        raise ValueError(\"summary_length must be a positive integer.\")\n",
    "    \n",
    "    if not text.strip():\n",
    "        return \"\"  \n",
    "    \n",
    "    summary_sentences = retrieve_top_sentences(text, top_k=summary_length, ensure_diversity=ensure_diversity, diversity_threshold=diversity_threshold)  \n",
    "    \n",
    "    original_sentences = sent_tokenize(text)  \n",
    "    ordered_summary = []\n",
    "    \n",
    "    for orig_sent in original_sentences:\n",
    "        clean_orig = re.sub(r'[^a-zA-Z\\s]', '', orig_sent)  \n",
    "        if any(re.sub(r'[^a-zA-Z\\s]', '', sum_sent) == clean_orig for sum_sent in summary_sentences):\n",
    "            ordered_summary.append(orig_sent)  \n",
    "    \n",
    "    if not ordered_summary:\n",
    "        ordered_summary = summary_sentences \n",
    "    summary = \" \".join(ordered_summary) \n",
    "    \n",
    "    return summary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4d0a2a6-66ed-4222-af80-f7adc3d387ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(generated_summary, reference_summary, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)  \n",
    "    generated_embedding = embeddings.embed_query(generated_summary) \n",
    "    reference_embedding = embeddings.embed_query(reference_summary) \n",
    "    \n",
    "    similarity = cosine_similarity([generated_embedding], [reference_embedding])  \n",
    "    \n",
    "    return similarity[0][0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e24e093b-efaf-4c76-adb0-42ab25bf5833",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mArtificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintelligent agents\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martificial intelligence\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is often used to describe machines that mimic cognitive functions that humans associate with the human mind, such as learning and problem-solving. As machines become increasingly capable, tasks considered to require \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintelligence\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m are often removed from the definition of AI, a phenomenon known as the AI effect. A quip in Tesler\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Theorem says \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI is whatever hasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt been done yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      5\u001b[0m summary_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \n\u001b[1;32m----> 6\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_diversity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiversity_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, summary)\n\u001b[0;32m      9\u001b[0m reference_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI demonstrates intelligence by machines, contrasting human intelligence. It is defined as the study of intelligent agents that maximize their chances of achieving goals. The term is often used to describe machines mimicking cognitive functions like learning and problem-solving. As machines improve, tasks requiring intelligence are often redefined, known as the AI effect. Optical character recognition is an example of a technology that has become routine and is no longer considered AI.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m, in \u001b[0;36mgenerate_summary\u001b[1;34m(text, summary_length, ensure_diversity, diversity_threshold)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[1;32m----> 8\u001b[0m summary_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_top_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_diversity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_diversity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiversity_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiversity_threshold\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Retrieve top sentences\u001b[39;00m\n\u001b[0;32m     10\u001b[0m original_sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(text)  \n\u001b[0;32m     11\u001b[0m ordered_summary \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m, in \u001b[0;36mretrieve_top_sentences\u001b[1;34m(text, top_k, ensure_diversity, diversity_threshold)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_top_sentences\u001b[39m(text, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, ensure_diversity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, diversity_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     processed_sentences, original_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m top_k \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(processed_sentences):\n\u001b[0;32m      5\u001b[0m         top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(processed_sentences)  \n",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      9\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer() \n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m---> 12\u001b[0m     words \u001b[38;5;241m=\u001b[39m word_tokenize(\u001b[43mclean_sentence\u001b[49m)\n\u001b[0;32m     13\u001b[0m     words \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m important_words \u001b[38;5;28;01melse\u001b[39;00m word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m     14\u001b[0m     filtered_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m filtered_stop_words]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term \"artificial intelligence\" is often used to describe machines that mimic cognitive functions that humans associate with the human mind, such as learning and problem-solving. As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect. A quip in Tesler's Theorem says \"AI is whatever hasn't been done yet.\" For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.\n",
    "\"\"\"\n",
    "\n",
    "summary_length = 5  \n",
    "summary = generate_summary(text, summary_length=summary_length, ensure_diversity=True, diversity_threshold=0.7)\n",
    "print(\"Summary:\\n\", summary)\n",
    "\n",
    "reference_summary = \"AI demonstrates intelligence by machines, contrasting human intelligence. It is defined as the study of intelligent agents that maximize their chances of achieving goals. The term is often used to describe machines mimicking cognitive functions like learning and problem-solving. As machines improve, tasks requiring intelligence are often redefined, known as the AI effect. Optical character recognition is an example of a technology that has become routine and is no longer considered AI.\"\n",
    "accuracy = calculate_accuracy(summary, reference_summary)\n",
    "print(\"Accuracy (Cosine Similarity):\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab6ba2-e880-4334-bea7-c04ef375ea32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
